# ğŸ“˜ Bielik Ethnographic Benchmark

Testowanie polskich modeli jÄ™zykowych w kontekÅ›cie wiedzy etnologicznej i kultury lokalnej.

---

## ğŸ” Cel projektu

Stworzenie benchmarku porÃ³wnujÄ…cego jakoÅ›Ä‡ odpowiedzi rÃ³Å¼nych modeli jÄ™zykowych (np. GPT-4, Gemini, Bielik) na pytania zamkniÄ™te z zakresu etnologii i historii spoÅ‚ecznej w Polsce.

---

## ğŸ§  Architektura i logika dziaÅ‚ania

Projekt jest modularny i rozdzielony na logiczne komponenty:

### ğŸ”¹ Struktura backendÃ³w (komunikacja z modelami)
- `llm_connector.py` â€“ gÅ‚Ã³wny punkt wejÅ›cia: funkcja `ask_model(config)` deleguje zapytanie do odpowiedniego backendu.
- `local_backend.py` â€“ obsÅ‚uga modeli lokalnych (np. Bielik z Hugging Face Transformers).
- `api_backend.py` â€“ obsÅ‚uga modeli przez API (OpenAI, Gemini, Hugging Face Inference API).

Backend wybierany jest dynamicznie na podstawie pola `api` w `model_config`.

### ğŸ”¹ Konfiguracja modelu
- Wszystkie parametry modelu (id, typ API, dÅ‚ugoÅ›Ä‡ odpowiedzi, URL, klucz API, kwantyzacja) przekazywane sÄ… przez argumenty CLI i trafiajÄ… do jednej struktury: `model_config`.

### ğŸ”¹ ObsÅ‚uga wyjÄ…tkÃ³w
- KaÅ¼dy backend posiada wÅ‚asnÄ… obsÅ‚ugÄ™ bÅ‚Ä™dÃ³w (brak odpowiedzi, timeouty, zÅ‚e dane wejÅ›ciowe, bÅ‚Ä™dne API key itp.).
- GÅ‚Ã³wna pÄ™tla benchmarku nie przerywa dziaÅ‚ania w przypadku bÅ‚Ä™du jednego zapytania.

### ğŸ”¹ Raportowanie
- Czas wykonania benchmarku i liczba przetworzonych pytaÅ„ sÄ… wypisywane po zakoÅ„czeniu dziaÅ‚ania.
- Wyniki zapisywane sÄ… do pliku `.jsonl` (lista odpowiedzi) i `.json` (podsumowanie).

---


## ğŸ—‚ï¸ Struktura repozytorium

```
â”œâ”€â”€ benchmark_test_llm_main.py        # GÅ‚Ã³wny skrypt uruchamiajÄ…cy testowanie modeli
â”œâ”€â”€ benchmark_merge_results.py        # Skrypt scalajÄ…cy i oceniajÄ…cy odpowiedzi modeli
â”‚
â”œâ”€â”€ moduÅ‚y/                           # GÅ‚Ã³wne komponenty systemu
â”‚   â”œâ”€â”€ dataset_loader.py             # Wczytywanie danych testowych z pliku CSV/XLSX
â”‚   â”œâ”€â”€ llm_connector.py              # Delegator: wybiera odpowiedni backend w zaleÅ¼noÅ›ci od konfiguracji
â”‚   â”œâ”€â”€ local_backend.py              # ObsÅ‚uga modeli lokalnych (np. Hugging Face, Bielik)
â”‚   â”œâ”€â”€ api_backend.py                # ObsÅ‚uga modeli przez API (OpenAI, Gemini, HF Inference API)
â”‚   â”œâ”€â”€ response_saver.py             # Zapis wynikÃ³w do JSON/JSONL
â”‚   â””â”€â”€ utils.py                      # Funkcje pomocnicze (parsowanie outputu, budowa promptu)
â”‚
â”œâ”€â”€ results/                          # Folder z odpowiedziami modeli i podsumowaniami
â”‚   â”œâ”€â”€ <llm_id>_raw.json             # Surowe odpowiedzi modelu (JSON lub JSONL)
â”‚   â””â”€â”€ <llm_id>_summary.json         # Podsumowanie poprawnoÅ›ci odpowiedzi
â”‚
â”œâ”€â”€ tests/                            # Testy jednostkowe i integracyjne
â”‚   â”œâ”€â”€ unit/                         # Testy funkcji pomocniczych i backendÃ³w
â”‚   â””â”€â”€ integration/                  # Testy peÅ‚nych przepÅ‚ywÃ³w dziaÅ‚ania skryptÃ³w
â”‚
â”œâ”€â”€ _natalia_prototyp/               # Archiwum pierwszej wersji benchmarku (nieuÅ¼ywane)
â”‚
â”œâ”€â”€ requirements.txt                 # Lista wymaganych bibliotek
â””â”€â”€ README.md                        # Niniejszy plik
```

---

## â–¶ï¸ Uruchamianie benchmarku - generowanie odpowiedzi

### Wersja CLI:

```bash
python benchmark_test_llm_main.py \
  --llm="bielik-chat" \
  --llm-name="Bielik" \
  --test="./test_files/test.xlsx" \
  --results="./results/bielik.json" \
  --api="local"  # dostÄ™pne: local, openAI, google, hf_api
  --interval=0
```
Po uruchomieniu benchmarku zapisywana jest lista surowych odpowiedzi modelu. 
PorÃ³wnanie odpowiedzi z prawidÅ‚owymi i statystyki sÄ… generowane w osobnym kroku (skrypt `benchmark_merge_results.py`).

### Wymagane argumenty:

- `--llm` â€“ unikalny identyfikator modelu
- `--llm-name` â€“ przyjazna nazwa modelu
- `--test` â€“ Å›cieÅ¼ka do pliku testowego (`.csv` lub `.xlsx`)

### Opcjonalne:

- `--results` â€“ Å›cieÅ¼ka do pliku wyjÅ›ciowego
- `--api` â€“ typ API (`local`, `openAI`, `vllm`)
- `--url`, `--key` â€“ jeÅ›li uÅ¼ywasz modelu przez API (np. OpenAI)
- `--interval` â€“ opÃ³Åºnienie miÄ™dzy zapytaniami

---

## ğŸ§ª Dane wejÅ›ciowe (testy)

Plik testowy powinien zawieraÄ‡ kolumny:

- `Pytanie`
- `A`, `B`, `C`, `D` â€“ moÅ¼liwe odpowiedzi
- `Pozycja` â€“ poprawna odpowiedÅº (litera A-D)
- `Domena`, `Kategoria`, `Tagi`

---

## ğŸ“¤ Dane wyjÅ›ciowe

Po uruchomieniu benchmarku zapisuje:

- `results/model_raw.json` â€“ surowe odpowiedzi modelu na kaÅ¼de pytanie (bez oceny)
- (w kolejnym kroku) `results/model_summary.json` â€“ podsumowanie ocen (tworzone osobnym skryptem)

---

## ğŸ“ Tworzenie promptu i przetwarzanie odpowiedzi

- Prompt budowany jest na podstawie kaÅ¼dego wiersza z pliku testowego, zgodnie z szablonem zdefiniowanym w `utils.py` (`PROMPT_TEMPLATE`).
- Odpowiedzi modelu sÄ… parsowane funkcjÄ… `parse_output()` z `utils.py` i zapisywane w surowej formie do pliku JSON przez `response_saver.py`.
- Ocena poprawnoÅ›ci i podsumowanie wynikÃ³w odbywa siÄ™ w kolejnym kroku, przez osobny skrypt (`benchmark_merge_results.py`).

---
## ğŸ§ª Testowanie

Projekt zawiera pokrycie testami jednostkowymi i integracyjnymi:

### âœ… Testy jednostkowe
- Dla kaÅ¼dego backendu (`local_backend`, `api_backend`, `llm_connector`).
- ObejmujÄ…:
  - poprawnoÅ›Ä‡ dziaÅ‚ania delegacji,
  - obsÅ‚ugÄ™ wyjÄ…tkÃ³w (np. bÅ‚Ä™dne configi, brak API key, bÅ‚Ä™dy sieci),
  - parsowanie odpowiedzi,
  - przekazywanie argumentÃ³w.

### âœ… Testy integracyjne
- `benchmark_test_llm_main.py` â€“ testuje peÅ‚en przebieg generowania odpowiedzi.


### ğŸ§ª Przypadki brzegowe
Testy obejmujÄ… m.in.:
- brak wymaganych pÃ³l w `model_config`,
- nieobsÅ‚ugiwany typ API (`invalid_api`),
- wyjÄ…tki rzucane przez modele (np. `TimeoutError`, `OpenAIError`),
- nieoczekiwany format odpowiedzi (np. brak `choices[0].message.content`).

### ğŸ”§ Mockowanie
- Testy uÅ¼ywajÄ… `mock` i `monkeypatch`, co pozwala symulowaÄ‡ zachowanie modeli bez realnego API.

### ğŸ“ CzytelnoÅ›Ä‡
- KaÅ¼dy test posiada `docstring` z opisem celu testu.
- Pliki testowe znajdujÄ… siÄ™ w folderze `tests/`.

---

## âœ… Status projektu

- âœ… Modularna architektura backendÃ³w
- âœ… ObsÅ‚uga: `local`, `openAI`, `google`, `hf_api`
- âœ… Czytelna struktura promptÃ³w i wynikÃ³w
- âœ… ObsÅ‚uga wyjÄ…tkÃ³w i bÅ‚Ä™dÃ³w sieciowych
- âœ… Pokrycie testami jednostkowymi i integracyjnymi
- ğŸ”œ Planowane: scalony raport porÃ³wnawczy dla wielu modeli (`benchmark_merge_results.py`)

---

## ğŸ”§ Wymagania

- Python 3.9+
- Biblioteki: `pandas`, `openai`, `google-generativeai`, `argparse`, `dotenv`, `requests`

Instalacja:

```bash
pip install -r requirements.txt
```

---

## ğŸ‘¥ ZespÃ³Å‚

- Natalia Nadolna (https://github.com/NataliaNadolna)
- Anna ZieliÅ„ska (https://github.com/ReviIsCoding)
- Krzysztof Raszczuk â€“ konsultacje merytoryczne

---
